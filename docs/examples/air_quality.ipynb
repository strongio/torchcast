{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c2fdd-79e7-452c-a0fe-3e3cf64f743f",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b4d50f",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torchcast.state_space import Predictions\n",
    "from torchcast.utils.datasets import load_air_quality_data\n",
    "from torchcast.kalman_filter import KalmanFilter\n",
    "from torchcast.utils.data import TimeSeriesDataset\n",
    "\n",
    "from plotnine import facet_wrap\n",
    "\n",
    "np.random.seed(2025-3-12)\n",
    "torch.manual_seed(2025-3-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd953c",
   "metadata": {},
   "source": [
    "# Multivariate Forecasts: Beijing Multi-Site Air-Quality Data\n",
    "\n",
    "We'll demonstrate several features of `torchcast` using a dataset from the [UCI Machine Learning Data Repository](https://archive.ics.uci.edu/dataset/501/beijing+multi+site+air+quality+data). It includes data on air pollutants and weather from 12 sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f73fa9",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df_aq = load_air_quality_data('weekly')\n",
    "\n",
    "SPLIT_DT = np.datetime64('2016-02-22')\n",
    "df_aq['dataset'] = np.where(df_aq['week'] > SPLIT_DT, 'val', 'train')\n",
    "df_aq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66408930",
   "metadata": {
    "hidePrompt": true
   },
   "source": [
    "### Univariate Forecasts\n",
    "\n",
    "Let's try to build a model to predict total particulate-matter (PM2.5 and PM10). \n",
    "\n",
    "First, we'll make our target the sum of these two types. We'll log-transform since this is strictly positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba2730",
   "metadata": {
    "lines_to_next_cell": 1,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchcast.process import LocalTrend, Season\n",
    "\n",
    "# dataset:\n",
    "df_aq['PM'] = df_aq['PM10'] + df_aq['PM2p5'] \n",
    "df_aq['PM_log10'] = np.log10(df_aq['PM']) \n",
    "dataset_pm = TimeSeriesDataset.from_dataframe(\n",
    "    dataframe=df_aq,\n",
    "    dt_unit='W',\n",
    "    measure_colnames=['PM_log10'],\n",
    "    group_colname='station', \n",
    "    time_colname='week'\n",
    ")\n",
    "dataset_pm_train, _ = dataset_pm.train_val_split(dt=SPLIT_DT)\n",
    "\n",
    "# model:\n",
    "kf_pm = KalmanFilter(\n",
    "    measures=['PM_log10'], \n",
    "    processes=[\n",
    "        LocalTrend(id='trend'),\n",
    "        Season(\n",
    "            id='day_in_year', \n",
    "            period=365.25 / 7, \n",
    "            dt_unit='W', \n",
    "            K=5,\n",
    "            fixed=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# fit:\n",
    "kf_pm.fit(\n",
    "    dataset_pm_train.tensors[0],\n",
    "    start_offsets=dataset_pm_train.start_datetimes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93acf9",
   "metadata": {},
   "source": [
    "Let's see how our forecasts look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc93da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper for transforming log back to original:\n",
    "def inverse_transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # bias-correction for log-transform (see https://otexts.com/fpp2/transformations.html#bias-adjustments)\n",
    "    df['mean'] = df['mean'] + .5 * df['std'] ** 2\n",
    "    df['lower'] = df['mean'] - 1.96 * df['std']\n",
    "    df['upper'] = df['mean'] + 1.96 * df['std']\n",
    "    # inverse the log10:\n",
    "    df[['actual', 'mean', 'upper', 'lower']] = 10 ** df[['actual', 'mean', 'upper', 'lower']]\n",
    "    df['measure'] = df['measure'].str.replace('_log10', '')\n",
    "    return df\n",
    "\n",
    "# generate forecasts:\n",
    "forecast = kf_pm(\n",
    "        dataset_pm_train.tensors[0],\n",
    "        start_offsets=dataset_pm_train.start_datetimes,\n",
    "        out_timesteps=dataset_pm.tensors[0].shape[1]\n",
    ")\n",
    "\n",
    "df_uv_pred = (forecast\n",
    "             .set_metadata(group_colname='station', time_colname='week')\n",
    "             .to_dataframe(dataset_pm, conf=None)\n",
    "             .pipe(inverse_transform))\n",
    "forecast.plot(\n",
    "    df_uv_pred, \n",
    "    max_num_groups=2, \n",
    "    split_dt=SPLIT_DT, \n",
    "    figure_size=(6,5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8216b",
   "metadata": {},
   "source": [
    "### Multivariate Forecasts\n",
    "\n",
    "Can we improve our model by splitting the pollutant we are forecasting into its two types (2.5 and 10), and modeling them in a multivariate manner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a11649",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# create a dataset:\n",
    "df_aq['PM10_log10'] = np.log10(df_aq['PM10'])\n",
    "df_aq['PM2p5_log10'] = np.log10(df_aq['PM2p5'])\n",
    "dataset_pm_multivariate = TimeSeriesDataset.from_dataframe(\n",
    "    dataframe=df_aq,\n",
    "    dt_unit='W',\n",
    "    measure_colnames=['PM10_log10','PM2p5_log10'],\n",
    "    group_colname='station', \n",
    "    time_colname='week'\n",
    ")\n",
    "dataset_pm_multivariate_train, _ = dataset_pm_multivariate.train_val_split(dt=SPLIT_DT)\n",
    "\n",
    "# create a model:\n",
    "_processes = []\n",
    "for m in dataset_pm_multivariate.measures[0]:\n",
    "    _processes.extend([\n",
    "        LocalTrend(id=f'{m}_trend', measure=m),\n",
    "        Season(id=f'{m}_day_in_year', period=365.25 / 7, dt_unit='W', K=5, measure=m, fixed=True)\n",
    "    ])\n",
    "kf_pm_multivariate = KalmanFilter(measures=dataset_pm_multivariate.measures[0], processes=_processes)\n",
    "\n",
    "# fit:\n",
    "kf_pm_multivariate.fit(\n",
    "    dataset_pm_multivariate_train.tensors[0],\n",
    "    start_offsets=dataset_pm_multivariate_train.start_datetimes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b6e9f",
   "metadata": {},
   "source": [
    "We can generate our one-month-ahead predictions for validation as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29dcd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    forecast_mv = kf_pm_multivariate(\n",
    "        dataset_pm_multivariate_train.tensors[0],\n",
    "        start_offsets=dataset_pm_multivariate_train.start_datetimes,\n",
    "        out_timesteps=dataset_pm_multivariate.num_timesteps # <--- how we ask it to forecast past original times\n",
    "    )\n",
    "forecast_mv.means.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432e2106",
   "metadata": {},
   "source": [
    "At this point, though, we run into a problem: we we have forecasts for both PM2.5 and PM10, but we ultimately want a forecast for their *sum*. With untransformed data, we could take advantage of the fact that [sum of correlated normals is still normal](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables#Correlated_random_variables); but we have log-transformed our measures. This seems like it was the right choice (i.e. our residuals look reasonably normal and i.i.d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b401f1-2614-417e-a123-57560b071cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_mv.plot(\n",
    "    forecast_mv.to_dataframe(dataset_pm_multivariate, type='components').query(\"process=='residuals'\"),\n",
    "    time_colname='time', group_colname='group',\n",
    "    figure_size=(6,5)\n",
    ") + facet_wrap('measure', ncol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6befb2",
   "metadata": {},
   "source": [
    "In this case, we can't take the sum of our forecasts to get the forecast of the sum, and [there's no simple closed-form expression for the sum of lognormals](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C14&q=SUMS+OF+LOGNORMALS&btnG=).\n",
    "\n",
    "One option that is fairly easy in `torchcast` is to use a [Monte-Carlo](https://en.wikipedia.org/wiki/Monte_Carlo_method) approach: we'll just generate random-samples based on the means and covariances underlying our forecast. In that case, the sum of the PM2.5 + PM10 forecasted-samples *is* the forecasted PM sum we are looking for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb82dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_preds_to_dataframe(preds: Predictions,\n",
    "                          dataset: TimeSeriesDataset,\n",
    "                          inverse_transform_fun: callable,\n",
    "                          num_draws: int = 1000,\n",
    "                          **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Our predictions are on the transformed scale, and we'd like to sum across measures on the original scale;\n",
    "    this function uses a monte-carlo approach to do this.\n",
    "    \"\"\"\n",
    "    # generate draws from the forecast distribution, apply inverse-transform:\n",
    "    mc_draws = inverse_transform_fun(torch.distributions.MultivariateNormal(*preds).rsample((num_draws,)))\n",
    "    # sum across measures (e.g. 2.5 and 10), then mean across draws:\n",
    "    mc_predictions = mc_draws.sum(-1, keepdim=True).mean(0)\n",
    "    # convert to a dataframe\n",
    "    return TimeSeriesDataset.tensor_to_dataframe(\n",
    "        mc_predictions,\n",
    "        times=dataset.times(),\n",
    "        group_names=dataset.group_names,\n",
    "        measures=['predicted'],\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011168d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mv_pred = mc_preds_to_dataframe(\n",
    "    forecast_mv,\n",
    "    dataset_pm_multivariate,\n",
    "    inverse_transform_fun=lambda x: 10 ** x,\n",
    "    group_colname='station',\n",
    "    time_colname='week'\n",
    ")\n",
    "df_mv_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea437c0",
   "metadata": {},
   "source": [
    "### Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06488417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_error(df: pd.DataFrame, pred_colname: str = 'mean', actual_colname: str = 'actual') -> pd.DataFrame:\n",
    "    return (df\n",
    "             .assign(sq_error=lambda df: (df[pred_colname] - df[actual_colname]) ** 2)\n",
    "             .groupby(['station'])\n",
    "             ['sq_error'].mean()\n",
    "             .reset_index()\n",
    "             .assign(rmse= lambda df: df['sq_error'] ** .5))\n",
    "\n",
    "def agg_within_group_err(df: pd.DataFrame, error_col : str = 'rmse', group_col: str = 'station') -> pd.DataFrame:\n",
    "    df = df.copy(deep=False)\n",
    "    df['error_norm'] = df[error_col] - df.groupby(group_col)[error_col].transform('mean')\n",
    "    return df.groupby('model').agg(**{error_col : (error_col, 'mean'), 'sem' : ('error_norm', 'sem')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe26330",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fct_err = pd.concat([\n",
    "            df_uv_pred\n",
    "             .loc[lambda df: df['week'] > SPLIT_DT,:]\n",
    "             .pipe(get_station_error)\n",
    "             .assign(model='univariate')\n",
    "            ,\n",
    "            df_mv_pred\n",
    "             .loc[lambda df: df['week'] > SPLIT_DT,:]\n",
    "             .merge(df_aq.loc[:, ['station', 'week', 'PM']])\n",
    "             .pipe(get_station_error, pred_colname='predicted', actual_colname='PM')\n",
    "             .assign(model='multivariate')\n",
    "]).reset_index(drop=True)\n",
    "df_fct_err.sort_values('station').tail(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fcc3b0",
   "metadata": {},
   "source": [
    "We see that this approach has reduced our error substantially in the validation period (though at the cost of slightly increasing it in the training period). We can look at the per-site differences to reduce noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1925cb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fct_err_agg = agg_within_group_err(df_fct_err)\n",
    "# https://chris-said.io/2014/12/01/independent-t-tests-and-the-83-confidence-interval-a-useful-trick-for-eyeballing-your-data/\n",
    "_multi = -stats.norm.ppf((1 - .83) / 2)\n",
    "\n",
    "df_fct_err_agg['rmse'].plot(\n",
    "    kind='bar', \n",
    "    yerr=df_fct_err_agg['sem'] * _multi, \n",
    "    ylabel=\"Root Sq. Err\\n(lower is better)\", \n",
    "    title=\"Forecasting Error\",\n",
    "    ylim=(77,83)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c4919",
   "metadata": {},
   "source": [
    "### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd679c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    one_mo_uv = kf_pm(\n",
    "        dataset_pm.tensors[0],\n",
    "        start_offsets=dataset_pm.start_datetimes,\n",
    "        n_step=4\n",
    "    )\n",
    "    one_mo_mv = kf_pm_multivariate(\n",
    "        dataset_pm_multivariate.tensors[0],\n",
    "        start_offsets=dataset_pm_multivariate.start_datetimes,\n",
    "        n_step=4\n",
    "    )\n",
    "    \n",
    "df_backtest_err1mo = pd.concat([\n",
    "\n",
    "    one_mo_uv\n",
    "    .to_dataframe(dataset_pm, time_colname='week', group_colname='station', conf=None)\n",
    "    .pipe(inverse_transform)\n",
    "    .loc[lambda df: df['week'] > SPLIT_DT, :]\n",
    "    .pipe(get_station_error)\n",
    "    .assign(model='univariate')\n",
    "    ,\n",
    "\n",
    "    mc_preds_to_dataframe(one_mo_mv,\n",
    "                          dataset_pm_multivariate,\n",
    "                          inverse_transform_fun=lambda x: 10 ** x,\n",
    "                          group_colname='station',\n",
    "                          time_colname='week')\n",
    "    .loc[lambda df: df['week'] > SPLIT_DT,:]\n",
    "    .merge(df_aq[['station', 'week', 'PM']])\n",
    "    .pipe(get_station_error, pred_colname='predicted', actual_colname='PM')\n",
    "    .assign(model='multivariate')\n",
    "])\n",
    "\n",
    "df_backtest1mo_err_agg = agg_within_group_err(df_backtest_err1mo)\n",
    "\n",
    "df_backtest1mo_err_agg['rmse'].plot(\n",
    "    kind='bar', \n",
    "    yerr=df_backtest1mo_err_agg['sem'], \n",
    "    ylabel=\"Root Sq. Err\\n(lower is better)\", \n",
    "    title=\"Backtesting (1-Month-Ahead) Error\",\n",
    "    ylim=(77,83)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f98e0",
   "metadata": {},
   "source": [
    "### Adding Predictors\n",
    "\n",
    "In many settings we have external (a.k.a. _exogenous_) predictors we'd like to incorporate. Here we'll use four predictors corresponding to weather conditions. Of course, in a forecasting context, we run into the problem of needing to fill in values for these predictors for future dates. For an arbitrary forecast horizon this can be a complex issue; for simplicity here we'll focus on the 4-week-ahead predictions we used above, and simply lag our weather predictors by 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5709beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcast.process import LinearModel\n",
    "\n",
    "# prepare external predictors:\n",
    "predictors_raw = ['TEMP', 'PRES', 'DEWP']\n",
    "predictors = [p.lower() + '_lag4' for p in predictors_raw]\n",
    "# standardize:\n",
    "predictor_means = df_aq.query(\"dataset=='train'\")[predictors_raw].mean()\n",
    "predictor_stds = df_aq.query(\"dataset=='train'\")[predictors_raw].std()\n",
    "df_aq[predictors] = (df_aq[predictors_raw] - predictor_means) / predictor_stds\n",
    "# lag:\n",
    "df_aq[predictors] = df_aq.groupby('station')[predictors].shift(4, fill_value=0)\n",
    "\n",
    "# create dataset:\n",
    "dataset_pm_lm = TimeSeriesDataset.from_dataframe(\n",
    "    dataframe=df_aq,\n",
    "    dt_unit='W',\n",
    "    y_colnames=['PM10_log10','PM2p5_log10'],\n",
    "    X_colnames=predictors,\n",
    "    group_colname='station', \n",
    "    time_colname='week',\n",
    ")\n",
    "dataset_pm_lm_train, _ = dataset_pm_lm.train_val_split(dt=SPLIT_DT)\n",
    "dataset_pm_lm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b48473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model:\n",
    "_processes = []\n",
    "for m in dataset_pm_lm.measures[0]:\n",
    "    _processes.extend([\n",
    "        LocalTrend(id=f'{m}_trend', measure=m),\n",
    "        Season(id=f'{m}_day_in_year', period=365.25 / 7, dt_unit='W', K=5, measure=m, fixed=True),\n",
    "        LinearModel(id=f'{m}_lm', predictors=predictors, measure=m)\n",
    "    ])\n",
    "kf_pm_lm = KalmanFilter(measures=dataset_pm_lm.measures[0], processes=_processes)\n",
    "\n",
    "# fit:\n",
    "y, X = dataset_pm_lm_train.tensors\n",
    "kf_pm_lm.fit(\n",
    "    y,\n",
    "    X=X, # if you want to supply different predictors to different processes, you can use `{process_name}__X`\n",
    "    start_offsets=dataset_pm_lm_train.start_datetimes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ecdf8",
   "metadata": {},
   "source": [
    "Here we show how to inspect the influence of each predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dad919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect components:\n",
    "with torch.no_grad():\n",
    "    y, X = dataset_pm_lm.tensors\n",
    "    one_mo_lm = kf_pm_lm(\n",
    "        y,\n",
    "        X=X,\n",
    "        start_offsets=dataset_pm_lm.start_datetimes,\n",
    "        n_step=4\n",
    "    )\n",
    "one_mo_lm.plot(\n",
    "    one_mo_lm\n",
    "    .to_dataframe(dataset_pm_lm, type='components')\n",
    "    .query(\"process.str.contains('lm')\")\n",
    "    .query(\"time > '2014-01-01'\"),\n",
    "    split_dt=SPLIT_DT, \n",
    "    figure_size=(6,5)\n",
    ") + facet_wrap('measure', ncol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d84a2",
   "metadata": {},
   "source": [
    "Now let's look at the change in error from our earlier multivariate model vs. one that includes predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest1mo_err_agg2 = agg_within_group_err(\n",
    "    pd.concat([\n",
    "        # old ones:\n",
    "        df_backtest_err1mo,\n",
    "        # new one:\n",
    "        mc_preds_to_dataframe(one_mo_lm,\n",
    "                              dataset_pm_lm,\n",
    "                              inverse_transform_fun=lambda x: 10 ** x,\n",
    "                              group_colname='station',\n",
    "                              time_colname='week')\n",
    "        .loc[lambda df: df['week'] > SPLIT_DT,:]\n",
    "        .merge(df_aq[['station', 'week', 'PM']])\n",
    "        .pipe(get_station_error, pred_colname='predicted', actual_colname='PM')\n",
    "        .assign(model='mv_with_preds')\n",
    "    ])\n",
    ")\n",
    "\n",
    "df_backtest1mo_err_agg2['rmse'].plot(\n",
    "    kind='bar', \n",
    "    yerr=df_backtest1mo_err_agg2['sem'], \n",
    "    ylabel=\"Root Sq. Err\\n(lower is better)\", \n",
    "    title=\"Backtesting (1-Month-Ahead) Error\",\n",
    "    ylim=(77,83)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44fd87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
